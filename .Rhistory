(0.25-0.3) + qt(0.975,7) * sqrt(1/6+1/3)
(0.25-0.3) - qt(0.975,7) * sqrt(1/6+1/3)
(6*1.1 5 *0.8+7*1)/18
(6*1.1 +5 *0.8+7*1)/18
qnorm(0.05)
qnorm(0.95)
qnorm(0.95)*3
qchisq(0.9,4)
qchisq(0.9,4)*9/4
qt(0.95,4)
qf(0.95, 1, 4)
3*sqrt(2)*qnorm(0.95)
qnorm(0.95)
qt(0.95,4)
qt(0.95,4)*sqrt(2)
qf(0.95,1,4)
qf(0.95,1,4)*4/(2*9)
pnorm(0.36337, lower.tail = F)
-0.05 + 2.365 * 0.1772 * sqrt(1/6+1/3)
-0.05 - 2.365 * 0.1772 * sqrt(1/6+1/3)
qf(0.95, 1,4)
qf(0.95, 1,4) * 4 / (2 * 9)
qchisq(0.9,4)
qchisq(0.9,4)*9/4
qchisq(0.05)
qchisq(0.05,df=10)
qchisq(0.05/99,df=10)
qchisq(0.05/99,df=10,lower.tail = F)
qchisq(0.05/99,df=10)
19^2 + 19^2
19.82880283^2 +  19.89936909^2
19.41217597^2 +  22.71295827^2
qchisq(0.05,df=10,lower.tail = F)
qchisq(0.05/99,df=10,lower.tail = F)
?qgamma()
qgamma(0.05, 100*10/2, 2/100)
qgamma(0.95, 100*10/2, 2/100)
qgamma(0.95, shape = 100*10/2, scale = 2/100)
qgamma(0.95, shape = 100*10/2, scale = 2/100)
qgamma(0.95, shape = 1000*10/2, scale = 2/1000)
qgamma(1 - 0.05/99, shape = 1000*10/2, scale = 2/1000)
qgamma(1 - 0.01/99, shape = 1000*10/2, scale = 2/1000)
qgamma(1 - 0.05/99, shape = 1000*10/2, scale = 2/1000)
library(igraphdata)
library(igraph)
library(dplyr)
library(reticulate)
data("enron", package = "igraphdata")
network_igraph <- upgrade_graph(enron) # network_igraph <- enron
rm(enron)
network_edgelist <- as.data.frame(as_edgelist(network_igraph))
network_edgelist <- cbind(network_edgelist, Time = E(network_igraph)$Time)
View(network_edgelist)
# remove data before 1990
removal <- which(network_edgelist$Time < as.Date("1990-01-01"))
network_edgelist <- network_edgelist[-removal,]
rm(network_igraph, removal)
View(network_edgelist)
# check time range
min(network_edgelist$Time)
max(network_edgelist$Time)
start_date <- end_date <- as.Date("2000-06-05")
num_of_week <- 100
for(time_iter in 1:num_of_week){
end_date <- end_date + 7
}; rm(time_iter)
# get frequent user
network_edgelist <- network_edgelist[which(network_edgelist$Time >= start_date &
network_edgelist$Time < end_date),]
all_users <- c(network_edgelist$V1, network_edgelist$V2)
all_users <- table(all_users) %>% as.data.frame() %>% arrange(desc(Freq)) # sorting
top_users <- all_users[1:100,] # select top 100 frequent user
selected_users <- droplevels(top_users$all_users)
rm(all_users, top_users, end_date)
y_list <- list()
edge_sum <- c()
seq_date <- seq(as.Date("2000-06-05"), as.Date("2002-05-06"), by="weeks")
seq_date <- seq_date[1:100] # remove the last one
seq_date
data("enron", package = "igraphdata")
network_igraph <- upgrade_graph(enron) # network_igraph <- enron
rm(enron)
network_edgelist <- as.data.frame(as_edgelist(network_igraph))
network_edgelist <- cbind(network_edgelist, Time = E(network_igraph)$Time)
# remove data before 1990
removal <- which(network_edgelist$Time < as.Date("1990-01-01"))
network_edgelist <- network_edgelist[-removal,]
rm(network_igraph, removal)
# check time range
min(network_edgelist$Time)
max(network_edgelist$Time)
start_date <- end_date <- as.Date("2000-06-05")
num_of_week <- 100
for(time_iter in 1:num_of_week){
end_date <- end_date + 7
}; rm(time_iter)
# get frequent user
network_edgelist <- network_edgelist[which(network_edgelist$Time >= start_date &
network_edgelist$Time < end_date),]
View(network_edgelist)
# Load necessary libraries
library(ggplot2)
# Set up data for normal distributions
x <- seq(-4, 4, length.out = 100)
# Normal distribution (mean = 2, sd = 1.5)
normal_dist <- dnorm(x, mean = 2, sd = 1.5)
# Standard normal distribution (mean = 0, sd = 1)
standard_normal_dist <- dnorm(x, mean = 0, sd = 1)
# Create data frame for ggplot
df <- data.frame(
x = rep(x, 2),
y = c(normal_dist, standard_normal_dist),
dist = rep(c("Normal (mean=2, sd=1.5)", "Standard Normal (mean=0, sd=1)"), each = 100)
)
# Plot Normal and Standard Normal distributions
p1 <- ggplot(df, aes(x = x, y = y, color = dist)) +
geom_line(size = 1) +
labs(title = "Normal and Standard Normal Distribution", y = "Density") +
theme_minimal()
# Chi-squared distribution (df = 3)
x_chi <- seq(0, 10, length.out = 100)
chi_squared_dist <- dchisq(x_chi, df = 3)
df_chi <- data.frame(x = x_chi, y = chi_squared_dist)
# Plot Chi-squared distribution
p2 <- ggplot(df_chi, aes(x = x, y = y)) +
geom_line(color = "blue", size = 1) +
labs(title = "Chi-squared Distribution (df = 3)", y = "Density") +
theme_minimal()
# t-distribution (df = 5)
x_t <- seq(-4, 4, length.out = 100)
t_dist <- dt(x_t, df = 5)
df_t <- data.frame(x = x_t, y = t_dist)
# Plot t-distribution
p3 <- ggplot(df_t, aes(x = x, y = y)) +
geom_line(color = "green", size = 1) +
labs(title = "t Distribution (df = 5)", y = "Density") +
theme_minimal()
# F-distribution (df1 = 5, df2 = 10)
x_f <- seq(0, 5, length.out = 100)
f_dist <- df(x_f, df1 = 5, df2 = 10)
df_f <- data.frame(x = x_f, y = f_dist)
# Plot F-distribution
p4 <- ggplot(df_f, aes(x = x, y = y)) +
geom_line(color = "red", size = 1) +
labs(title = "F Distribution (df1 = 5, df2 = 10)", y = "Density") +
theme_minimal()
# Display plots
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
data <- read.table("http://www.stat.ucla.edu/~nchristo/statistics_c173_c273/jura.txt",header=TRUE)
y <- data$Pb
x1 <- data$Cd
x2 <- data$Co
x3 <- data$Cr
x4 <- data$Cu
x5 <- data$Ni
x6 <- data$Zn
X <- as.matrix(cbind(rep(1, nrow(data)), x1,x2,x3,x4,x5,x6))
# Estimation and Se^2
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
se2 <- (t(y) %*% y - t(beta_hat) %*% t(X) %*% y)/(nrow(data)-6-1)
V <- solve(t(X) %*% X)
F_stat <- (beta_hat[5] - 0)^2 / (se2 * V[5,5])
pf(F_stat, 1, nrow(data)-6-1, lower.tail = F)
T_stat <- (beta_hat[5] - 0) / (se2^0.5 * V[5,5]^0.5)
2*pt(T_stat, nrow(data)-6-1, lower.tail = F) # same p-value
F_stat
T_stat <- (beta_hat[5] - 0) / (se2^0.5 * V[5,5]^0.5)
2*pt(T_stat, nrow(data)-6-1, lower.tail = F) # same p-value
summary(lm(y~x1+x2+x3+x4+x5+x6)) # validation via lm()
T_stat
# Set the number of nodes (n) and dimension of the feature vector (d)
n <- 10  # number of nodes
d <- 5   # dimension of each feature vector
# Step 1: Create the matrix nu (n x d)
set.seed(123)  # Set seed for reproducibility
nu <- matrix(rnorm(n * d), nrow = n, ncol = d)  # Random matrix of size n x d
print("Original matrix nu:")
print(nu)
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(1:(n-1), 2:n)  # Edges between consecutive nodes
View(edges)
E <- nrow(edges)              # Number of edges
sample(10)
sample(10, replace = T)
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
View(edges)
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
View(edges)
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
View(edges)
sum(edges[,1]==edges[,2])
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
if(sum(edges[,1]==edges[,2]) > 0){
edges <- cbind(sample(10, replace = T), sample(10, replace = T))
}
View(edges)
sum(edges[,1]==edges[,2])
E <- nrow(edges)              # Number of edges
# Construct the incidence matrix A (n x |E|)
A <- matrix(0, nrow = n, ncol = E)
k<-1
# Construct the incidence matrix A (n x |E|)
A <- matrix(0, nrow = n, ncol = E)
A
i <- edges[k, 1]
j <- edges[k, 2]
A[i, k] <- 1
A[j, k] <- -1
A
# Construct the incidence matrix A (n x |E|)
A <- matrix(0, nrow = n, ncol = E)
for (k in 1:E) {
i <- edges[k, 1]
j <- edges[k, 2]
A[i, k] <- 1
A[j, k] <- -1
}
A
# Convert to sparse matrix (optional for efficiency with larger graphs)
A <- as(Matrix(A, sparse = TRUE), "dgCMatrix")
print("Incidence matrix A:")
# Load required library
library(Matrix)
# Convert to sparse matrix (optional for efficiency with larger graphs)
A <- as(Matrix(A, sparse = TRUE), "dgCMatrix")
print("Incidence matrix A:")
print(A)
# Step 3: Choose the initial vector gamma (the feature vector of the first node)
gamma <- nu[1, ]
print("Initial vector gamma (nu_1):")
print(gamma)
# Step 4: Compute the difference matrix beta (|E| x d)
beta <- matrix(0, nrow = E, ncol = d)
for (k in 1:E) {
i <- edges[k, 1]
j <- edges[k, 2]
beta[k, ] <- nu[i, ] - nu[j, ]
}
print(beta)
# Step 4: Compute the difference matrix beta (|E| x d)
beta <- matrix(0, nrow = E, ncol = d)
for (k in 1:E) {
i <- edges[k, 1]
j <- edges[k, 2]
beta[k, ] <- nu[i, ] - nu[j, ]
}
print("Difference matrix beta (nu_i - nu_j for edges):")
print(beta)
# Step 5: Reconstruct the matrix nu from gamma and beta using A
nu_reconstructed <- matrix(0, nrow = n, ncol = d)
for (i in 1:n) {
nu_reconstructed[i, ] <- gamma + as.numeric(A[i, ]) %*% beta
}
print("Reconstructed matrix nu:")
print(nu_reconstructed)
# Check if the original nu and the reconstructed nu are approximately equal
print("Difference between original and reconstructed nu:")
print(nu - nu_reconstructed)
# Load required library
library(Matrix)
# Set the number of nodes (n) and dimension of the feature vector (d)
n <- 10  # number of nodes
d <- 5   # dimension of each feature vector
# Step 1: Create the matrix nu (n x d)
set.seed(123)  # Set seed for reproducibility
nu <- matrix(rnorm(n * d), nrow = n, ncol = d)  # Random matrix of size n x d
print("Original matrix nu:")
print(nu)
# Create a simple graph structure by defining edges
# For simplicity, let's assume it's a line graph (node 1 connected to 2, 2 to 3, ..., n-1 to n)
edges <- cbind(sample(10, replace = T), sample(10, replace = T))  # Edges between consecutive nodes
if(sum(edges[,1]==edges[,2]) > 0){
edges <- cbind(sample(10, replace = T), sample(10, replace = T))
}
E <- nrow(edges)              # Number of edges
# Construct the incidence matrix A (n x |E|)
A <- matrix(0, nrow = n, ncol = E)
for (k in 1:E) {
i <- edges[k, 1]
j <- edges[k, 2]
A[i, k] <- 1
A[j, k] <- -1
}
A
# Convert to sparse matrix (optional for efficiency with larger graphs)
A <- as(Matrix(A, sparse = TRUE), "dgCMatrix")
print("Incidence matrix A:")
print(A)
# Step 3: Choose the initial vector gamma (the feature vector of the first node)
gamma <- nu[1, ]
print("Initial vector gamma (nu_1):")
print(gamma)
# Step 4: Compute the difference matrix beta (|E| x d)
beta <- matrix(0, nrow = E, ncol = d)
for (k in 1:E) {
i <- edges[k, 1]
j <- edges[k, 2]
beta[k, ] <- nu[i, ] - nu[j, ]
}
print("Difference matrix beta (nu_i - nu_j for edges):")
print(beta)
nu_reconstructed <- matrix(0, nrow = n, ncol = d)
nu_reconstructed[1, ] <- gamma  # The first row is the reference vector gamma
# Reconstruct remaining rows by cumulatively adding the differences (beta)
for (i in 2:n) {
# Each row i is the previous row plus the difference from the previous edge
nu_reconstructed[i, ] <- nu_reconstructed[i-1, ] + beta[i-1, ]
}
print("Reconstructed matrix nu:")
print(nu_reconstructed)
# Check if the original nu and the reconstructed nu are approximately equal
print("Difference between original and reconstructed nu:")
print(nu - nu_reconstructed)
nu_reconstructed <- matrix(0, nrow = n, ncol = d)
nu_reconstructed[1, ] <- gamma  # The first row is the reference vector gamma
sqrt(0.5)
365 * 364
factorial(365) / (factorial(365-30) *365^30 )
factorial(3)
factorial(365)/(365^30)
35^2 / 38 + 37/38
4/(38^2)
1 - (2/38)^2
2/38
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
# Step 2: Estimate the density using kernel density estimation
kernel_density <- density(data)
?density
# Step 3: Plot both the estimated density and the true density
# Define the true density function for a normal distribution
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Plot the kernel density estimate
plot(kernel_density, main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density", lwd = 2, col = "blue")
# Add the true density curve
curve(true_density, add = TRUE, col = "red", lwd = 2, lty = 2)
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
gaussian_kernel <- function(u) {
return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, h) {
n <- length(data)
kernel_values <- sapply(data, function(d) gaussian_kernel((x - d) / h))
return(sum(kernel_values) / (n * h))
}
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
gaussian_kernel <- function(u) {
return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, h) {
n <- length(data)
kernel_values <- sapply(data, function(d) gaussian_kernel((x - d) / h))
return(sum(kernel_values) / (n * h))
}
# Step 3: Compute KDE at a range of x values
x_values <- seq(-4, 4, length.out = 1000)
kde_values <- sapply(x_values, function(x) kde(x, data, h = 0.1))
# Step 4: Plot both the kernel density estimate and the true density
# Define the true density function for a normal distribution
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Plot the kernel density estimate
plot(x_values, kde_values, type = "l", col = "blue", lwd = 2,
main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density")
# Add the true density curve
curve(true_density(x), add = TRUE, col = "red", lwd = 2, lty = 2)
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
gaussian_kernel <- function(u) {
return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, h) {
n <- length(data)
kernel_values <- sapply(data, function(d) gaussian_kernel((x - d) / h))
return(sum(kernel_values) / (n * h))
}
# Step 3: Compute KDE at a range of x values
#x_values <- seq(-4, 4, length.out = 1000)
kde_values <- sapply(data, function(x) kde(x, data, h = 0.1))
# Step 4: Plot both the kernel density estimate and the true density
# Define the true density function for a normal distribution
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Plot the kernel density estimate
plot(x_values, kde_values, type = "l", col = "blue", lwd = 2,
main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density")
# Step 4: Plot both the kernel density estimate and the true density
# Define the true density function for a normal distribution
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Plot the kernel density estimate
plot(data, kde_values, type = "l", col = "blue", lwd = 2,
main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density")
sort(data)
# Step 3: Compute KDE at a range of x values
#x_values <- seq(-4, 4, length.out = 1000)
kde_values <- sapply(sort(data), function(x) kde(x, data, h = 0.1))
# Step 4: Plot both the kernel density estimate and the true density
# Define the true density function for a normal distribution
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Plot the kernel density estimate
plot(data, kde_values, type = "l", col = "blue", lwd = 2,
main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density")
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
# Gaussian kernel function
gaussian_kernel <- function(u) {
return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, h) {
n <- length(data)
kernel_values <- sapply(data, function(d) gaussian_kernel((x - d) / h))
return(sum(kernel_values) / (n * h))
}
min(data)
max(data)
# Step 2: Create a sequence of x-values for plotting the KDE smoothly
x_values <- seq(min(data), max(data) , length.out = 1000)  # Generate x-values in the range of data
kde_values <- sapply(x_values, function(x) kde(x, data, h = 0.1))  # Compute KDE at these points
# Step 3: Define the true density function for comparison (normal distribution)
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
# Step 4: Plot both the kernel density estimate and the true density
plot(x_values, kde_values, type = "l", col = "blue", lwd = 2,
main = "Kernel Density Estimation vs True Density",
xlab = "x", ylab = "Density")
# Add the true density curve
curve(true_density(x), add = TRUE, col = "red", lwd = 2, lty = 2)
# Step 1: Generate sample data
set.seed(123)  # Set seed for reproducibility
n <- 1000  # Number of samples
mu <- 0  # Mean of the normal distribution
sigma <- 1  # Standard deviation of the normal distribution
# Generate data from a normal distribution
data <- rnorm(n, mean = mu, sd = sigma)
# Gaussian kernel function
gaussian_kernel <- function(u) {
return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, h) {
n <- length(data)
kernel_values <- sapply(data, function(d) gaussian_kernel((x - d) / h))
return(sum(kernel_values) / (n * h))
}
# Step 2: Create a sequence of x-values for plotting the KDE smoothly
#x_values <- seq(min(data), max(data) , length.out = 1000)  # Generate x-values in the range of data
kde_values <- sapply(data, function(x) kde(x, data, h = 0.1))  # Compute KDE at these points
# Step 3: Define the true density function for comparison (normal distribution)
true_density <- function(x) {
dnorm(x, mean = mu, sd = sigma)
}
d_true <- true_density(data)
mean((kde_values - d_true)^2)
setwd("~/Documents/GitHub/CPDmrdpg")
